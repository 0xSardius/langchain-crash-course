{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is it?\n",
    "    - RAG, or Retrieval Augmented Generation, is a technique that enhances Large Language Models (LLMs) by incorporating additional data beyond what the model was trained on.\n",
    "- RAG relies on the following core components:\n",
    "    - Documents: RAG processes documents by splitting them into smaller chunks for better indexing and retrieval.\n",
    "    - Document Loaders: These load data for indexing in RAG applications.\n",
    "    - Text Splitters: They break down large documents into manageable chunks for indexing and model input.\n",
    "    - Embedding Models: Used to embed text chunks for indexing and retrieval in RAG.\n",
    "        - An embedding is a numerical representation of a word's meaning.\n",
    "    - Vector Stores: Store and index the embedded text chunks for efficient retrieval.\n",
    "    - Retrievers: Retrieve relevant data from the vector store based on user queries for the model to generate answers.\n",
    "- Why is it important?\n",
    "    - RAG is crucial as it allows LLMs to incorporate new data beyond their training set, enabling them to reason about private or post-training data.\n",
    "- What problem does it solve for us?\n",
    "    -  It solves the problem of limited knowledge in LLMs by dynamically adding specific information needed for accurate responses.\n",
    "- Additional use cases with examples.\n",
    "    - Q&A on large documents.\n",
    "    - Q&A on knowledge bases.\n",
    "    - Q&A on code repositories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Document Loaders:\n",
    "    - TextLoader\n",
    "    - CSVLoader\n",
    "    - WebBaseLoader\n",
    "    - YoutubeLoader\n",
    "    - NotionDB\n",
    "\n",
    "- Text Splitters:\n",
    "    - CharacterTextSplitter\n",
    "    - RecursiveCharacterTextSplitter\n",
    "    - MarkdownTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Load the Document: Read the text content from your file\n",
    "loader = TextLoader(\"state_of_the_union.txt\")  \n",
    "# Replace \"state_of_the_union.txt\" with the path to your text file\n",
    "documents = loader.load()  \n",
    "\n",
    "# Split the Document into Chunks: Make the text easier to manage\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# Split into chunks of 1000 characters with no overlap between chunks\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Display information about the split documents\n",
    "print(f\"Number of document chunks: {len(docs)}\")\n",
    "print(f\"Sample chunk:\\n{docs[0].page_content}\")  # Show the content of the first chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Embedding Pricing:\n",
    "- https://openai.com/api/pricing/\n",
    "- $0.02 per 1M tokens\n",
    "\n",
    "Chroma:\n",
    "- Chroma is an open-source embedding database specifically designed for AI applications. It allows you to store, index, and retrieve embedded representations of your text data (or other vectorized data) efficiently. Think of it as a specialized database for embeddings, optimized for the kinds of similarity searches you need in natural language processing tasks.\n",
    "- Alternatives:\n",
    "    - FAISS, Pinecone, Weaviate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Create Embeddings: Turn text into numbers so the computer can understand meaning\n",
    "embeddings = OpenAIEmbeddings()  # Uses OpenAI's embedding model (requires your API key)\n",
    "\n",
    "# Create Vector Store: A database to store the embeddings for efficient searching\n",
    "db = Chroma.from_documents(docs, embeddings)  # Embed each chunk and store in the Chroma database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User's Question: What the user wants to know\n",
    "query = \"What did the president say about the economy?\"\n",
    "\n",
    "# Retrieve Relevant Documents: Find the most relevant parts of the text based on the query\n",
    "retriever = db.as_retriever(search_type=\"similarity\") \n",
    "# Use the vector store to find similar documents based on meaning\n",
    "relevant_docs = retriever.get_relevant_documents(query, k=3) # Get the top 3 most relevant documents\n",
    "\n",
    "# TODO: Show similarity scores for each document\n",
    "# # Search the DB.\n",
    "# results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "# if len(results) == 0 or results[0][1] < 0.7:\n",
    "#     print(f\"Unable to find matching results.\")\n",
    "#     return\n",
    "\n",
    "# Display the Relevant Results\n",
    "print(f\"Relevant Documents:\\n\")\n",
    "for doc in relevant_docs:\n",
    "    print(doc.page_content)  # Print the content of each relevant document chunk\n",
    "\n",
    "\n",
    "# TODO: Show how to pass chunks into prompt template and use it to generate the response.\n",
    "# https://github.com/pixegami/langchain-rag-tutorial/blob/main/query_data.py\n",
    "# context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "# prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "# prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "# print(prompt)\n",
    "# model = ChatOpenAI()\n",
    "# response_text = model.predict(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- Show how to add citations.\n",
    "- Show how to add metadata while embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DuckDB + Parquet:\n",
    "\n",
    "In the chromadb.config.Settings, we used chroma_db_impl=\"duckdb+parquet\". Here's why this combination is often used:\n",
    "\n",
    "DuckDB: As an in-memory database, DuckDB offers excellent performance for smaller to medium-sized datasets. It's very fast for data loading, indexing, and querying.\n",
    "Parquet: Parquet is a columnar storage format that is highly efficient for storing and reading large datasets. It also supports compression, which can help reduce storage costs.\n",
    "By combining DuckDB with Parquet, you get the best of both worlds:\n",
    "\n",
    "Fast in-memory processing for interactive queries and small datasets.\n",
    "Efficient storage and retrieval for larger datasets thanks to Parquet's columnar format.\n",
    "You can choose other combinations depending on your needs:\n",
    "\n",
    "duckdb: Use only DuckDB (good for in-memory, smaller datasets).\n",
    "parquet: Use only Parquet files (suitable for larger datasets but might be slower for interactive queries).\n",
    "\n",
    "\n",
    "ConversationBufferMemory:\n",
    "\n",
    "Purpose: Stores the history of messages exchanged in a conversation. This includes both the user's queries and the AI's responses.\n",
    "Alternatives: LangChain offers other memory options like:\n",
    "ConversationSummaryMemory: Summarizes the conversation periodically.\n",
    "ConversationEntityMemory: Tracks specific entities mentioned in the conversation.\n",
    "ConversationKGMemory: Stores information as a knowledge graph.\n",
    "\n",
    "\n",
    "Alternatives to chain_type=\"stuff\":\n",
    "\n",
    "In the RetrievalQA chain, the chain_type parameter determines how the retrieved documents are incorporated into the prompt for the language model. Here are the common options:\n",
    "\n",
    "- stuff: Inserts all retrieved documents directly into the prompt, one after the other.\n",
    "- map_reduce: Summarizes each document individually and then combines the summaries.\n",
    "- refine: Uses the first retrieved document to generate an initial response, then iteratively refines the response using subsequent documents.\n",
    "- map_rerank: Generates a score for each retrieved document based on its relevance to the query and then selects the top-scoring documents.\n",
    "\n",
    "How Developers Typically Use Collections:\n",
    "\n",
    "There's no one-size-fits-all answer, but here are some common approaches:\n",
    "\n",
    "- By Source: Divide collections based on the source of the data (e.g., \"books,\" \"articles,\" \"code\").\n",
    "- By Topic: Group documents related to similar topics into collections (e.g., \"machine learning,\" \"finance,\" \"health\").\n",
    "- By Time: If your data has a temporal dimension, you can create collections for different time periods (e.g., \"2023 news,\" \"2022 reports\").\n",
    "- By Granularity: Sometimes, you might have documents that naturally fit into different levels of granularity. For example, you could have a collection for entire books and another collection for individual chapters.\n",
    "- By Author: If you are working with text documents from a variety of authors, it can be helpful to split things up by author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continual Chat\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "\n",
    "# Your OpenAI API Key\n",
    "OPENAI_API_KEY = 'YOUR_OPENAI_API_KEY'\n",
    "\n",
    "# 1. Load and Prepare Your Data \n",
    "loader = TextLoader(\"state_of_the_union.txt\") \n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)  \n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 2. Set Up Your Chroma Database \n",
    "persist_directory = \"db\"  \n",
    "client_settings = Settings(chroma_db_impl=\"duckdb+parquet\", persist_directory=persist_directory)\n",
    "client = chromadb.Client(client_settings)\n",
    "collection = client.get_or_create_collection(name=\"my_documents\") \n",
    "\n",
    "if collection.count() == 0:\n",
    "    collection.add(\n",
    "        documents=[doc.page_content for doc in docs],  \n",
    "        metadatas=[doc.metadata for doc in docs], \n",
    "        ids=[f\"doc_{i}\" for i in range(len(docs))], \n",
    "    )\n",
    "\n",
    "# 3. Set Up the Question Answering System with History-Aware Retrieval \n",
    "retriever = collection.as_retriever()\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0)\n",
    "\n",
    "# Use a pre-configured prompt for retrieval QA (or define your own)\n",
    "chat_retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chain\")\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm,retriever=retriever, memory=memory, chain_type='stuff')\n",
    "\n",
    "# 4. Have a Conversation!\n",
    "while True:  \n",
    "    query = input(\"You: \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    result = qa_chain({\"question\": query}) \n",
    "    print(f\"AI: {result['answer']}\") \n",
    "\n",
    "\n",
    "\n",
    "# TODO: Use langchain.chains import create_history_aware_retriever\n",
    "# https://github.com/alejandro-ao/chat-with-websites/blob/master/src/app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

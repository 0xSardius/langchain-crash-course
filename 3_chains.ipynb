{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "- What is it?\n",
    "    - Chains in LangChain are sequences of calls, whether to a language model, a tool, or a data preprocessing step, providing a standard interface for constructing and executing these sequences efficiently\n",
    "- Why is it important?\n",
    "    - They are crucial as they allow for the creation of complex workflows, enabling the connection of language models to various data sources and utilities, enhancing the model's capabilities\n",
    "- What problem does it solve for us?\n",
    "    - Chains solve the problem of orchestrating multiple tasks in a structured manner, facilitating interactions with different tools and data sources seamlessly within an application\n",
    "- How do I build a chain? \n",
    "    - Runnables are chained together in Langchain by creating a sequence of runnables where the output of one runnable serves as the input to the next. This chaining can be achieved using the pipe operator (|) or the more explicit .pipe() method.\n",
    "- How is this possilbe?\n",
    "    - Because most classes in LangChain inherit from the \"Runnable\" class which means they \n",
    "- Desired end goal.\n",
    "    - \n",
    "- Additional use cases with examples.\n",
    "    - \n",
    "\n",
    "# Runnable\n",
    "- What is it?\n",
    "    - A runnable is a fundamental building block in Langchain that represents a task or operation that can be executed.\n",
    "    - It can transform a single input into an output, efficiently process multiple inputs in batches, or stream output as it's produced.\n",
    "    - Runnables can be synchronous or asynchronous, and they can accept configuration parameters to customize their behavior.\n",
    "- How do runnables relate to Chains?\n",
    "    - A chain in Langchain is a sequence of runnables that are executed in a specific order, where the output of one runnable serves as the input to the next.\n",
    "    - Chains allow for complex workflows to be built by composing individual runnables together.\n",
    "- Example:\n",
    "```python\n",
    "from langchain.runnables import Runnable\n",
    "\n",
    "# Define two simple runnables\n",
    "runnable1 = Runnable(lambda x: x * 2)\n",
    "runnable2 = Runnable(lambda x: x + 5)\n",
    "\n",
    "# Chain the two runnables together using the pipe operator\n",
    "chain = runnable1 | runnable2\n",
    "\n",
    "# Invoke the chain with an input value\n",
    "result = chain.invoke(3)\n",
    "\n",
    "print(result)  # Output will be 11 (3 * 2 + 5)```\n",
    "```\n",
    "- Example, keep growing chain:\n",
    "```python\n",
    "from langchain.runnables import Runnable\n",
    "\n",
    "# Define three runnables with different operations\n",
    "runnable1 = Runnable(lambda x: x * 2)\n",
    "runnable2 = Runnable(lambda x: x + 5)\n",
    "runnable3 = Runnable(lambda x: x ** 2)\n",
    "\n",
    "# Chain the runnables together using the pipe operator\n",
    "chain = runnable1 | runnable2 | runnable3\n",
    "\n",
    "# Invoke the chain with an input value\n",
    "result = chain.invoke(3)\n",
    "\n",
    "print(result)  # Output will be 64 ((3 * 2 + 5) ** 2)\n",
    "```\n",
    "\n",
    "\n",
    "# Summary\n",
    "A runnable is a single unit of work that can be executed, while a chain is a sequence of runnables that are orchestrated to perform a series of tasks in a specific order. Runnables and chains work together to enable the creation of flexible and scalable workflows in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example that uses what we've learned so far.\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core import Runnable\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = PromptTemplate(\"Tell me a joke about {topic}.\")\n",
    "\n",
    "# Define a chat model\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Create a chain by chaining the prompt template and chat model\n",
    "chain = prompt_template | chat_model\n",
    "\n",
    "# Invoke the chain with a topic\n",
    "result = chain.invoke({\"topic\": \"cats\"})\n",
    "\n",
    "print(result)  # Output will be a joke about cats generated by the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chains simplified with LangChain Expression Language (LCEL)\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Your OpenAI API key\n",
    "OPENAI_API_KEY = 'YOUR_OPENAI_API_KEY'\n",
    "\n",
    "# Initialize ChatOpenAI model\n",
    "chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.7)\n",
    "\n",
    "# Define prompt templates (no need for separate Runnable chains)\n",
    "joke_template = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a joke about {topic}.\"\n",
    ")\n",
    "follow_up_template = ChatPromptTemplate.from_template(\n",
    "    \"Why do {topic} like to sleep so much?\"\n",
    ")\n",
    "\n",
    "# Create the combined chain using LCEL\n",
    "chain = (\n",
    "    prompt_joke \n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    "    | follow_up_template \n",
    "    | chat_model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Get the topic for the joke and run the chain\n",
    "topic = input(\"What topic should the joke be about? \")\n",
    "response = chain.invoke({\"topic\": topic})\n",
    "\n",
    "# Present the results \n",
    "print(\"\\nJoke:\")\n",
    "print(response[\"output_1\"])  \n",
    "print(\"\\nExplanation:\")\n",
    "print(response[\"output_2\"])  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

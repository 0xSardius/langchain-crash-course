{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Takes in messages and generates output messages.\n",
    "\n",
    "Example input:\n",
    "```\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"id\": \"1\",\n",
    "      \"message\": \"Hello\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"2\",\n",
    "      \"message\": \"World\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='81 divided by 9 is 9.', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 25, 'total_tokens': 34}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_319be4768e', 'finish_reason': 'stop', 'logprobs': None}, id='run-5b5743bb-1988-47a6-93e3-0b8bd6faa4d4-0', usage_metadata={'input_tokens': 25, 'output_tokens': 9, 'total_tokens': 34})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base Case\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI # Talk about other ChatModels\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\") # Talk about other ChatGPT Models\n",
    "\n",
    "# Setup messagse\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "messages = [\n",
    "    SystemMessage(content=\"Solve the following math problems\"),\n",
    "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
    "]\n",
    "model.invoke(messages)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT Model alternatives:\n",
    "- \"gpt-3.5-turbo\"\n",
    "- \"gpt-4\"\n",
    "Full list here: https://platform.openai.com/docs/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatModel Alternatives:\n",
    "- Old list: https://python.langchain.com/v0.1/docs/integrations/chat/\n",
    "- See actual models, look inside the langchain.chat_models \n",
    "\n",
    "\"ChatOpenAI\",\n",
    "\"BedrockChat\",\n",
    "\"AzureChatOpenAI\",\n",
    "\"FakeListChatModel\",\n",
    "\"PromptLayerChatOpenAI\",\n",
    "\"ChatDatabricks\",\n",
    "\"ChatEverlyAI\",\n",
    "\"ChatAnthropic\",\n",
    "\"ChatCohere\",\n",
    "\"ChatGooglePalm\",\n",
    "\"ChatMlflow\",\n",
    "\"ChatMLflowAIGateway\",\n",
    "\"ChatOllama\",\n",
    "\"ChatVertexAI\",\n",
    "\"JinaChat\",\n",
    "\"HumanInputChatModel\",\n",
    "\"MiniMaxChat\",\n",
    "\"ChatAnyscale\",\n",
    "\"ChatLiteLLM\",\n",
    "\"ErnieBotChat\",\n",
    "\"ChatJavelinAIGateway\",\n",
    "\"ChatKonko\",\n",
    "\"PaiEasChatEndpoint\",\n",
    "\"QianfanChatEndpoint\",\n",
    "\"ChatFireworks\",\n",
    "\"ChatYandexGPT\",\n",
    "\"ChatBaichuan\",\n",
    "\"ChatHunyuan\",\n",
    "\"GigaChat\",\n",
    "\"VolcEngineMaasChat\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical Example\n",
    "# Example: Continually chat with the Chat Model \n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Solve the following math problems\"),\n",
    "]\n",
    "\n",
    "while True:\n",
    "    # Ask the user for their input\n",
    "    user_input = input(\"You: \")\n",
    "    new_message = HumanMessage(content=user_input)\n",
    "    messages.append(new_message)\n",
    "    model_response = model.invoke(messages)\n",
    "    print(\"Bot:\", model_response.content)\n",
    "    messages.append(SystemMessage(content=model_response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='81 divided by 9 equals 9.\\n\\nHere\\'s how we can solve this problem:\\n\\n81 รท 9 = ?\\n\\nTo divide 81 by 9, we can think of it as \"how many groups of 9 are there in 81?\"\\n\\nWe can write this out as a simple division problem:\\n\\n  9 | _ 81\\n      - 81\\n      ----\\n         0\\n\\nSo, there are 9 groups of 9 in 81, with no remainder. Therefore, 81 divided by 9 equals 9.', response_metadata={'id': 'msg_01JSLqz61qjQW5pjeWQ8Ab81', 'model': 'claude-3-opus-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 23, 'output_tokens': 134}}, id='run-938ac9e2-1894-464d-81ee-1a9b340bf4ae-0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anthropic Example\n",
    "# https://pypi.org/project/langchain-anthropic/\n",
    "# Anthropic models: https://docs.anthropic.com/en/docs/models-overview\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-opus-20240229\")\n",
    "\n",
    "messagse = [\n",
    "    SystemMessage(content=\"Solve the following math problems\"),\n",
    "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: \n",
    "- [ ] Add more examples.\n",
    "- [ ] What is it? Comparison to what they are already using.\n",
    "- [ ] Why do you need it and when to use it?\n",
    "- [ ] Show possibilities with more advanced functionality.\n",
    "        - If they want to learn more about this topic, comment down below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat History:\n",
    "- What is it?\n",
    "    - ChatMessageHistory is a utility class in Langchain that provides a simple way to manage the conversation history between a human and an AI.\n",
    "- Why is it important?\n",
    "    - Maintains context for the conversation, allowing the AI to understand and respond to follow-up questions.\n",
    "    - Enables persistent storage of the chat history, so the conversation can continue across multiple sessions\n",
    "- How to use it?\n",
    "    - \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# File History Example\n",
    "```python\n",
    "from langchain.memory import FileChatMessageHistory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# Set up the chat model and message history\n",
    "chat = ChatOpenAI(temperature=0.7)\n",
    "history = FileChatMessageHistory(file_path=\"chat_history.txt\")\n",
    "\n",
    "# Define the chat prompt templates\n",
    "system_template = \"\"\"You are a helpful AI assistant that answers questions based on the provided context. \n",
    "The user will provide you with a question, and you should respond with the most relevant information from the context.\n",
    "If the question cannot be answered from the provided context, say that you do not have enough information to answer the question.\n",
    "\"\"\"\n",
    "human_template = \"{question}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate(system_template),\n",
    "        HumanMessagePromptTemplate(human_template),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the retrieval QA chain with history\n",
    "qa_chain = create_retrieval_qa_with_history_chain(\n",
    "    llm=chat,\n",
    "    retriever=retriever,\n",
    "    prompt=chat_prompt,\n",
    "    memory=history,\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the purpose of LangChain?\"\n",
    "result = qa_chain({\"question\": query})\n",
    "print(result[\"result\"])\n",
    "\n",
    "# Add the query and response to the chat history\n",
    "history.add_user_message(query)\n",
    "history.add_ai_message(result[\"result\"])\n",
    "\n",
    "# Ask a follow-up question\n",
    "follow_up = \"Can you elaborate on the features of LangChain?\"\n",
    "result = qa_chain({\"question\": follow_up})\n",
    "print(result[\"result\"])\n",
    "\n",
    "history.add_user_message(follow_up)\n",
    "history.add_ai_message(result[\"result\"])\n",
    "```\n",
    "\n",
    "Simpler example:\n",
    "```python\n",
    "import json\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    ChatMessageHistory,\n",
    ")\n",
    "\n",
    "def save_history(history, filename=\"chat_history.json\"):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump([message.dict() for message in history.messages], f)\n",
    "\n",
    "def load_history(filename=\"chat_history.json\"):\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            messages = [\n",
    "                HumanMessage.parse_obj(message)\n",
    "                if message[\"type\"] == \"human\"\n",
    "                else AIMessage.parse_obj(message)\n",
    "                for message in json.load(f)\n",
    "            ]\n",
    "        return ChatMessageHistory(messages=messages)\n",
    "    except FileNotFoundError:\n",
    "        return ChatMessageHistory()\n",
    "\n",
    "# Initialize chat history, model, and system message\n",
    "history = load_history()  # Load history at the beginning\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "system_message = \"You are a helpful AI assistant.\"\n",
    "if not history.messages:  # Add system message only if history is empty\n",
    "    history.add_message(SystemMessage(content=system_message))\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"You: \")\n",
    "    if user_message.lower() == \"exit\":\n",
    "        break\n",
    "    history.add_message(HumanMessage(content=user_message))\n",
    "\n",
    "    ai_response = chat(history.messages)\n",
    "    history.add_message(AIMessage(content=ai_response.content))\n",
    "    print(f\"AI: {ai_response.content}\")\n",
    "\n",
    "# Save history at the end of the conversation\n",
    "save_history(history)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchaincc311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

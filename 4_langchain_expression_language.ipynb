{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LCEL change explanation:\n",
    "    - No Explicit Runnables: We no longer need to create separate RunnablePassthrough objects or combine them with the chat_model using the pipe operator (|). LCEL handles the chaining implicitly.\n",
    "    - Direct Chaining: The pipe operator (|) in LCEL directly connects the components, making the code much more readable and concise.\n",
    "    - Output Parser: We introduce StrOutputParser() to extract the string content from the ChatOpenAI output, which is in AIMessage format.\n",
    "    - Named Outputs: LCEL allows you to name the outputs of each step using output_1, output_2, etc., making it easier to access the results of each step in the final output dictionary.\n",
    "\n",
    "- Benifits of LCEL:\n",
    "    - Conciseness: The code is significantly shorter and easier to understand.\n",
    "    - Readability: The flow of data through the chain is more apparent.\n",
    "    - Flexibility: LCEL makes it easier to modify or extend the chain as needed.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple LCEL example\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Your OpenAI API Key\n",
    "OPENAI_API_KEY = 'YOUR_OPENAI_API_KEY'\n",
    "\n",
    "# Initialize the Chat Model (like hiring a smart assistant)\n",
    "chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.7)\n",
    "\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_template(\"Write a poem about {subject}.\")\n",
    "    | chat_model\n",
    "    | StrOutputParser()  \n",
    ")\n",
    "\n",
    "result = chain.invoke({\"subject\": \"nature\"})\n",
    "print(result)  # Prints the poem generated by the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded example that uses what we've learned so far.\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Your OpenAI API key\n",
    "OPENAI_API_KEY = 'YOUR_OPENAI_API_KEY'\n",
    "\n",
    "# Initialize ChatOpenAI model\n",
    "chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.7)\n",
    "\n",
    "# Define prompt templates (no need for separate Runnable chains)\n",
    "joke_template = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a joke about {topic}.\"\n",
    ")\n",
    "follow_up_template = ChatPromptTemplate.from_template(\n",
    "    \"Why do {topic} like to sleep so much?\"\n",
    ")\n",
    "\n",
    "# Create the combined chain using LCEL\n",
    "chain = (\n",
    "    prompt_joke \n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    "    | follow_up_template \n",
    "    | chat_model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Get the topic for the joke and run the chain\n",
    "topic = input(\"What topic should the joke be about? \")\n",
    "response = chain.invoke({\"topic\": topic})\n",
    "\n",
    "# Present the results \n",
    "print(\"\\nJoke:\")\n",
    "print(response[\"output_1\"])  \n",
    "print(\"\\nExplanation:\")\n",
    "print(response[\"output_2\"])  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
